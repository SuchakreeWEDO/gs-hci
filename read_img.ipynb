{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1194, 1600])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "img_path = r\"D:\\3d-reconstruction\\gs-hci\\output\\android-save-depth-8e39d121-8\\save_imgs\\20240219_154556\\20240219_154556_depth_3658.png\"\n",
    "\n",
    "# gs_depth = cv2.imread(img_path)\n",
    "# plt.imshow(gs_depth)\n",
    "# print(np.min(gs_depth))\n",
    "# print(np.max(gs_depth))\n",
    "# # image\n",
    "\n",
    "from PIL import Image\n",
    "from utils.general_utils import PILtoTorch\n",
    "\n",
    "gs_depth_img = Image.open(img_path)\n",
    "gs_depth = PILtoTorch(gs_depth_img,  gs_depth_img.size)\n",
    "print(gs_depth.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert_bit = cv2.bitwise_not(image) # OR\n",
    "# plt.imshow(invert_bit)\n",
    "# print(np.min(invert_bit))\n",
    "# print(np.max(invert_bit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert_sub = 255 - image\n",
    "# plt.imshow(invert_sub)\n",
    "# print(np.min(invert_sub))\n",
    "# print(np.max(invert_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monodepth_img = cv2.imread(r\"D:\\3d-reconstruction\\datasets\\android-img-fixisoae\\depth\\20240219_154556_depth.png\")\n",
    "# plt.imshow(monodepth_img)\n",
    "# print(monodepth_img.shape)\n",
    "# print(np.min(monodepth_img))\n",
    "# print(np.max(monodepth_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.Image.Image image mode=L size=3936x2939 at 0x24F6FA5A860>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1194, 1600])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "from utils.general_utils import PILtoTorch\n",
    "\n",
    "\n",
    "mono_depth_path = r\"D:\\3d-reconstruction\\datasets\\android-img-fixisoae\\depth\\20240219_154556_depth.png\"\n",
    "\n",
    "mono_depth = Image.open(mono_depth_path)\n",
    "mono_depth = ImageOps.grayscale(mono_depth) \n",
    "print(mono_depth)\n",
    "\n",
    "orig_w, orig_h = gs_depth_img.size\n",
    "\n",
    "resolution = -1\n",
    "resolution_scale = 1.0\n",
    "\n",
    "# lp.resolution\n",
    "if resolution == -1:\n",
    "    if orig_w > 1600:\n",
    "        print(\"[ INFO ] Encountered quite large input images (>1.6K pixels width), rescaling to 1.6K.\\n \"\n",
    "            \"If this is not desired, please explicitly specify '--resolution/-r' as 1\")\n",
    "        global_down = orig_w / 1600\n",
    "    else:\n",
    "        global_down = 1\n",
    "else:\n",
    "    global_down = orig_w / resolution\n",
    "\n",
    "scale = float(global_down) * float(resolution_scale)\n",
    "resolution = (int(orig_w / scale), int(orig_h / scale))\n",
    "\n",
    "\n",
    "\n",
    "resized_mono_depth = PILtoTorch(mono_depth, resolution)\n",
    "mono_depth = resized_mono_depth[:3, ...].squeeze(0)\n",
    "mono_depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\3d-reconstruction\\\\datasets\\\\android-img-fixisoae\\\\depth\\\\20240219_154556'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.join(r\"D:\\3d-reconstruction\\datasets\\android-img-fixisoae\", \"depth\", \"20240219_154556\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1194, 1600])\n"
     ]
    }
   ],
   "source": [
    "gs_depth.size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monodepth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
